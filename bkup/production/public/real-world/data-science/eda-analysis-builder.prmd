---
id: eda-analysis-builder
name: Exploratory Data Analysis Builder
version: 1.0.0
description: Comprehensive EDA with statistical analysis, visualization, and data quality assessment. Analyzes actual datasets when provided via context files.
tags: [data-analysis, eda, statistics, visualization]
parameters:
  - name: dataset_name
    type: string
    required: true
    description: Name/description of the dataset being analyzed
  - name: analysis_focus
    type: string
    enum: [business-insights, ml-preparation, data-quality, hypothesis-testing, all]
    default: all
    description: Primary focus of the analysis
  - name: data_type
    type: string
    enum: [tabular, time-series, text, images, mixed]
    required: true
    description: Primary data type
  - name: target_variable
    type: string
    required: false
    description: Target variable for supervised learning (if applicable)
  - name: business_context
    type: string
    required: false
    description: Business domain context (e.g., retail, healthcare, finance)
  - name: sample_size
    type: string
    enum: [small, medium, large, massive]
    default: medium
    description: Dataset size category
  - name: include_advanced_stats
    type: boolean
    default: true
    description: Include advanced statistical tests and analysis
---

# Exploratory Data Analysis: {{dataset_name}}

> **Dataset Analysis**: Run with `--meta:context` to analyze actual data files:
> ```bash
> prompd run eda-analysis-builder.prompd \
>   --meta:context ./data/customer_data.csv \
>   --meta:context ./data/data_dictionary.xlsx \
>   --meta:context ./notebooks/initial_exploration.ipynb
> ```

## Analysis Overview

**Dataset:** {{dataset_name}}  
**Data Type:** {{data_type}}  
**Focus:** {{analysis_focus}}  
**Sample Size:** {{sample_size}}  
{{#if target_variable}}**Target Variable:** {{target_variable}}{{/if}}  
{{#if business_context}}**Business Context:** {{business_context}}{{/if}}

{{#if meta.context}}
## Dataset Context Analysis

**Provided Files:** {{#each meta.context}}{{this}}{{#unless @last}}, {{/unless}}{{/each}}

### Initial Data Inspection
Based on the provided data files, I will:
1. **Load and examine data structure** from primary dataset
2. **Review data dictionary** for variable definitions (if provided)
3. **Analyze existing exploration** from notebooks (if provided)
4. **Generate comprehensive statistical profile** based on actual data patterns

{{/if}}

---

## EDA Framework & Setup

```python
# Core libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import warnings
warnings.filterwarnings('ignore')

# Statistical analysis
from scipy import stats
from scipy.stats import chi2_contingency, normaltest, shapiro
import statsmodels.api as sm
from statsmodels.stats.outliers_influence import variance_inflation_factor

# Advanced analysis
{{#if include_advanced_stats}}
from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans
from sklearn.ensemble import IsolationForest
{{/if}}

# Visualization styling
plt.style.use('seaborn-v0_8')
sns.set_palette("husl")

# Configuration
pd.set_option('display.max_columns', None)
pd.set_option('display.max_rows', 100)

# Create figure directory
import os
os.makedirs('eda_figures', exist_ok=True)

print("üîç Starting Comprehensive EDA for: {{dataset_name}}")
print("=" * 60)
```

---

## 1. Data Overview & Quality Assessment

```python
class DataProfiler:
    def __init__(self, df: pd.DataFrame):
        self.df = df
        self.profile_results = {}
        
    def basic_info(self):
        """Generate basic dataset information"""
        print("üìä DATASET BASIC INFO")
        print("-" * 40)
        print(f"Shape: {self.df.shape}")
        print(f"Memory Usage: {self.df.memory_usage(deep=True).sum() / 1024**2:.2f} MB")
        print()
        
        print("Data Types:")
        dtype_counts = self.df.dtypes.value_counts()
        for dtype, count in dtype_counts.items():
            print(f"  {dtype}: {count} columns")
        print()
        
        # Display first few rows
        print("First 5 rows:")
        display(self.df.head())
        
        # Display info
        print("\nDataFrame Info:")
        self.df.info(memory_usage='deep')
        
    def data_quality_report(self):
        """Comprehensive data quality assessment"""
        print("\nüîç DATA QUALITY ASSESSMENT")
        print("-" * 40)
        
        quality_report = []
        
        for col in self.df.columns:
            col_info = {
                'column': col,
                'dtype': str(self.df[col].dtype),
                'non_null_count': self.df[col].count(),
                'null_count': self.df[col].isnull().sum(),
                'null_percentage': (self.df[col].isnull().sum() / len(self.df)) * 100,
                'unique_values': self.df[col].nunique(),
                'unique_percentage': (self.df[col].nunique() / len(self.df)) * 100
            }
            
            # Additional checks for numeric columns
            if self.df[col].dtype in ['int64', 'float64']:
                col_info.update({
                    'mean': self.df[col].mean(),
                    'std': self.df[col].std(),
                    'min': self.df[col].min(),
                    'max': self.df[col].max(),
                    'zeros_count': (self.df[col] == 0).sum(),
                    'negative_count': (self.df[col] < 0).sum()
                })
                
            # Additional checks for categorical columns
            else:
                top_value = self.df[col].mode().iloc[0] if not self.df[col].empty else None
                col_info.update({
                    'top_value': top_value,
                    'top_value_freq': self.df[col].value_counts().iloc[0] if not self.df[col].empty else 0
                })
                
            quality_report.append(col_info)
            
        quality_df = pd.DataFrame(quality_report)
        
        # Display quality summary
        print(f"Total columns: {len(quality_df)}")
        print(f"Columns with missing values: {(quality_df['null_count'] > 0).sum()}")
        print(f"Columns with >50% missing: {(quality_df['null_percentage'] > 50).sum()}")
        print(f"Potential ID columns (>95% unique): {(quality_df['unique_percentage'] > 95).sum()}")
        print()
        
        # Missing values heatmap
        if self.df.isnull().sum().sum() > 0:
            plt.figure(figsize=(12, 8))
            sns.heatmap(self.df.isnull(), yticklabels=False, cbar=True, cmap='viridis')
            plt.title('Missing Values Heatmap')
            plt.tight_layout()
            plt.savefig('eda_figures/missing_values_heatmap.png', dpi=300, bbox_inches='tight')
            plt.show()
        
        return quality_df
        
    def detect_duplicates(self):
        """Detect and analyze duplicate records"""
        print("\nüîÑ DUPLICATE ANALYSIS")
        print("-" * 40)
        
        # Full duplicates
        full_duplicates = self.df.duplicated().sum()
        print(f"Full duplicate rows: {full_duplicates} ({full_duplicates/len(self.df)*100:.2f}%)")
        
        # Partial duplicates (excluding potential ID columns)
        non_id_cols = [col for col in self.df.columns 
                      if self.df[col].nunique() / len(self.df) < 0.95]
        
        if len(non_id_cols) > 1:
            partial_duplicates = self.df[non_id_cols].duplicated().sum()
            print(f"Partial duplicates (excluding ID cols): {partial_duplicates}")
            
        # Column-wise duplicate analysis
        duplicate_stats = []
        for col in self.df.columns:
            dup_count = self.df[col].duplicated().sum()
            duplicate_stats.append({
                'column': col,
                'duplicate_values': dup_count,
                'duplicate_percentage': dup_count / len(self.df) * 100
            })
            
        return pd.DataFrame(duplicate_stats).sort_values('duplicate_percentage', ascending=False)

# Execute data profiling
profiler = DataProfiler(df)
profiler.basic_info()
quality_df = profiler.data_quality_report()
duplicate_stats = profiler.detect_duplicates()

# Display quality summary table
print("\nDATA QUALITY SUMMARY:")
display(quality_df.head(10))
```

---

## 2. Univariate Analysis

```python
class UnivariateAnalyzer:
    def __init__(self, df: pd.DataFrame):
        self.df = df
        self.numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()
        self.categorical_cols = df.select_dtypes(include=['object', 'category']).columns.tolist()
        
    def analyze_numeric_variables(self):
        """Comprehensive analysis of numeric variables"""
        print("\nüìà NUMERIC VARIABLES ANALYSIS")
        print("-" * 40)
        
        if not self.numeric_cols:
            print("No numeric variables found.")
            return
            
        # Statistical summary
        desc_stats = self.df[self.numeric_cols].describe()
        print("Descriptive Statistics:")
        display(desc_stats)
        
        # Distribution analysis
        n_cols = min(len(self.numeric_cols), 4)
        n_rows = (len(self.numeric_cols) + n_cols - 1) // n_cols
        
        fig, axes = plt.subplots(n_rows, n_cols, figsize=(20, 5*n_rows))
        axes = axes.flatten() if n_rows > 1 else [axes] if n_cols == 1 else axes
        
        for i, col in enumerate(self.numeric_cols):
            if i < len(axes):
                # Histogram with KDE
                axes[i].hist(self.df[col].dropna(), bins=30, density=True, alpha=0.7, color='skyblue')
                
                # Add KDE if data is not constant
                if self.df[col].nunique() > 1:
                    self.df[col].dropna().plot(kind='kde', ax=axes[i], color='red', linewidth=2)
                
                axes[i].set_title(f'{col} Distribution')
                axes[i].set_xlabel(col)
                axes[i].set_ylabel('Density')
                
                # Add statistics text
                mean_val = self.df[col].mean()
                median_val = self.df[col].median()
                std_val = self.df[col].std()
                
                stats_text = f'Mean: {mean_val:.2f}\nMedian: {median_val:.2f}\nStd: {std_val:.2f}'
                axes[i].text(0.02, 0.98, stats_text, transform=axes[i].transAxes, 
                           verticalalignment='top', bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))
        
        # Hide empty subplots
        for i in range(len(self.numeric_cols), len(axes)):
            axes[i].set_visible(False)
            
        plt.tight_layout()
        plt.savefig('eda_figures/numeric_distributions.png', dpi=300, bbox_inches='tight')
        plt.show()
        
        # Box plots for outlier detection
        if len(self.numeric_cols) > 1:
            plt.figure(figsize=(15, 8))
            
            # Standardize data for comparison
            standardized_data = StandardScaler().fit_transform(self.df[self.numeric_cols].fillna(0))
            standardized_df = pd.DataFrame(standardized_data, columns=self.numeric_cols)
            
            sns.boxplot(data=standardized_df)
            plt.title('Standardized Box Plots (Outlier Detection)')
            plt.xticks(rotation=45)
            plt.tight_layout()
            plt.savefig('eda_figures/numeric_boxplots.png', dpi=300, bbox_inches='tight')
            plt.show()
            
        {{#if include_advanced_stats}}
        # Normality testing
        print("\nNORMALITY TESTS:")
        normality_results = []
        
        for col in self.numeric_cols:
            data = self.df[col].dropna()
            
            if len(data) > 3:
                # Shapiro-Wilk test (for smaller samples)
                if len(data) <= 5000:
                    shapiro_stat, shapiro_p = shapiro(data)
                else:
                    shapiro_stat, shapiro_p = None, None
                
                # D'Agostino-Pearson test
                dagostino_stat, dagostino_p = normaltest(data)
                
                normality_results.append({
                    'variable': col,
                    'shapiro_statistic': shapiro_stat,
                    'shapiro_p_value': shapiro_p,
                    'dagostino_statistic': dagostino_stat,
                    'dagostino_p_value': dagostino_p,
                    'is_normal_shapiro': shapiro_p > 0.05 if shapiro_p else None,
                    'is_normal_dagostino': dagostino_p > 0.05
                })
        
        normality_df = pd.DataFrame(normality_results)
        display(normality_df)
        {{/if}}
        
    def analyze_categorical_variables(self):
        """Comprehensive analysis of categorical variables"""
        print("\nüìä CATEGORICAL VARIABLES ANALYSIS")
        print("-" * 40)
        
        if not self.categorical_cols:
            print("No categorical variables found.")
            return
            
        # Value counts analysis
        for col in self.categorical_cols[:5]:  # Limit to first 5 for space
            print(f"\n{col.upper()} - Value Counts:")
            value_counts = self.df[col].value_counts()
            
            # Show top 10 values
            print(value_counts.head(10))
            
            # Calculate category statistics
            unique_count = self.df[col].nunique()
            most_frequent_pct = value_counts.iloc[0] / len(self.df) * 100
            
            print(f"Unique categories: {unique_count}")
            print(f"Most frequent category: '{value_counts.index[0]}' ({most_frequent_pct:.1f}%)")
            
            # Check for potential data quality issues
            if unique_count > len(self.df) * 0.5:
                print("‚ö†Ô∏è  High cardinality - consider grouping rare categories")
            
            # Check for similar category names
            categories = value_counts.index.astype(str)
            similar_cats = []
            for i, cat1 in enumerate(categories[:20]):  # Check top 20 categories
                for cat2 in categories[i+1:21]:
                    if cat1.lower().replace(' ', '') == cat2.lower().replace(' ', ''):
                        similar_cats.append((cat1, cat2))
                        
            if similar_cats:
                print(f"‚ö†Ô∏è  Potential similar categories found: {similar_cats[:3]}")
        
        # Visualization for categorical variables
        n_cols = min(len(self.categorical_cols), 2)
        n_rows = (len(self.categorical_cols) + n_cols - 1) // n_cols
        
        fig, axes = plt.subplots(n_rows, n_cols, figsize=(15, 6*n_rows))
        if n_rows == 1 and n_cols == 1:
            axes = [axes]
        elif n_rows == 1:
            axes = axes
        else:
            axes = axes.flatten()
        
        for i, col in enumerate(self.categorical_cols):
            if i < len(axes):
                # Top 10 categories
                top_categories = self.df[col].value_counts().head(10)
                
                if len(top_categories) > 5:
                    # Horizontal bar plot for many categories
                    top_categories.plot(kind='barh', ax=axes[i])
                    axes[i].set_title(f'{col} - Top 10 Categories')
                else:
                    # Vertical bar plot for few categories
                    top_categories.plot(kind='bar', ax=axes[i])
                    axes[i].set_title(f'{col} - Category Distribution')
                    axes[i].tick_params(axis='x', rotation=45)
                
                # Add percentage labels
                total = top_categories.sum()
                for j, (category, count) in enumerate(top_categories.items()):
                    pct = count / total * 100
                    if len(top_categories) > 5:
                        axes[i].text(count + max(top_categories)*0.01, j, f'{pct:.1f}%', 
                                   va='center', fontsize=10)
                    else:
                        axes[i].text(j, count + max(top_categories)*0.01, f'{pct:.1f}%', 
                                   ha='center', fontsize=10)
        
        # Hide empty subplots
        for i in range(len(self.categorical_cols), len(axes)):
            axes[i].set_visible(False)
            
        plt.tight_layout()
        plt.savefig('eda_figures/categorical_distributions.png', dpi=300, bbox_inches='tight')
        plt.show()

# Execute univariate analysis
univariate_analyzer = UnivariateAnalyzer(df)
univariate_analyzer.analyze_numeric_variables()
univariate_analyzer.analyze_categorical_variables()
```

---

## 3. Bivariate Analysis

```python
class BivariateAnalyzer:
    def __init__(self, df: pd.DataFrame, target_variable: str = None):
        self.df = df
        self.target_variable = target_variable
        self.numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()
        self.categorical_cols = df.select_dtypes(include=['object', 'category']).columns.tolist()
        
    def correlation_analysis(self):
        """Comprehensive correlation analysis"""
        print("\nüîó CORRELATION ANALYSIS")
        print("-" * 40)
        
        if len(self.numeric_cols) < 2:
            print("Need at least 2 numeric variables for correlation analysis.")
            return
            
        # Correlation matrix
        corr_matrix = self.df[self.numeric_cols].corr()
        
        # Visualization
        plt.figure(figsize=(12, 10))
        
        # Create mask for upper triangle
        mask = np.triu(np.ones_like(corr_matrix, dtype=bool))
        
        # Generate heatmap
        sns.heatmap(corr_matrix, mask=mask, annot=True, cmap='RdYlBu_r', center=0,
                   square=True, fmt='.2f', cbar_kws={"shrink": .8})
        
        plt.title('Correlation Matrix Heatmap')
        plt.tight_layout()
        plt.savefig('eda_figures/correlation_heatmap.png', dpi=300, bbox_inches='tight')
        plt.show()
        
        # High correlation pairs
        high_corr_pairs = []
        for i in range(len(corr_matrix.columns)):
            for j in range(i+1, len(corr_matrix.columns)):
                corr_val = corr_matrix.iloc[i, j]
                if abs(corr_val) > 0.7:  # High correlation threshold
                    high_corr_pairs.append({
                        'var1': corr_matrix.columns[i],
                        'var2': corr_matrix.columns[j],
                        'correlation': corr_val
                    })
        
        if high_corr_pairs:
            print("\n‚ö†Ô∏è  HIGH CORRELATIONS (|r| > 0.7):")
            for pair in high_corr_pairs:
                print(f"  {pair['var1']} ‚Üî {pair['var2']}: {pair['correlation']:.3f}")
        
        return corr_matrix
        
    def target_variable_analysis(self):
        """Analyze relationships with target variable"""
        if not self.target_variable or self.target_variable not in self.df.columns:
            print("\nNo target variable specified or found. Skipping target analysis.")
            return
            
        print(f"\nüéØ TARGET VARIABLE ANALYSIS: {self.target_variable}")
        print("-" * 50)
        
        target_is_numeric = self.target_variable in self.numeric_cols
        
        if target_is_numeric:
            self._analyze_numeric_target()
        else:
            self._analyze_categorical_target()
            
    def _analyze_numeric_target(self):
        """Analysis when target is numeric (regression)"""
        # Target distribution
        plt.figure(figsize=(15, 5))
        
        plt.subplot(1, 3, 1)
        self.df[self.target_variable].hist(bins=30, alpha=0.7, color='lightblue')
        plt.title(f'{self.target_variable} Distribution')
        plt.xlabel(self.target_variable)
        plt.ylabel('Frequency')
        
        plt.subplot(1, 3, 2)
        stats.probplot(self.df[self.target_variable].dropna(), dist="norm", plot=plt)
        plt.title(f'{self.target_variable} Q-Q Plot')
        
        plt.subplot(1, 3, 3)
        self.df[self.target_variable].plot(kind='box')
        plt.title(f'{self.target_variable} Box Plot')
        
        plt.tight_layout()
        plt.savefig('eda_figures/target_distribution.png', dpi=300, bbox_inches='tight')
        plt.show()
        
        # Correlations with target
        target_corrs = self.df[self.numeric_cols].corrwith(self.df[self.target_variable]).sort_values(
            key=abs, ascending=False
        )
        target_corrs = target_corrs.drop(self.target_variable, errors='ignore')
        
        print("TOP CORRELATIONS WITH TARGET:")
        print(target_corrs.head(10))
        
        # Scatter plots with top correlated features
        top_features = target_corrs.head(4).index
        
        if len(top_features) > 0:
            fig, axes = plt.subplots(2, 2, figsize=(15, 12))
            axes = axes.flatten()
            
            for i, feature in enumerate(top_features):
                if i < 4:
                    sns.scatterplot(data=self.df, x=feature, y=self.target_variable, 
                                  alpha=0.6, ax=axes[i])
                    
                    # Add trend line
                    z = np.polyfit(self.df[feature].dropna(), 
                                 self.df[self.target_variable].loc[self.df[feature].dropna().index], 1)
                    p = np.poly1d(z)
                    axes[i].plot(self.df[feature], p(self.df[feature]), "r--", alpha=0.8)
                    
                    axes[i].set_title(f'{feature} vs {self.target_variable}\nCorr: {target_corrs[feature]:.3f}')
            
            plt.tight_layout()
            plt.savefig('eda_figures/target_correlations.png', dpi=300, bbox_inches='tight')
            plt.show()
        
    def _analyze_categorical_target(self):
        """Analysis when target is categorical (classification)"""
        # Target distribution
        plt.figure(figsize=(12, 5))
        
        plt.subplot(1, 2, 1)
        target_counts = self.df[self.target_variable].value_counts()
        target_counts.plot(kind='bar')
        plt.title(f'{self.target_variable} Distribution')
        plt.xticks(rotation=45)
        
        # Add percentage labels
        total = target_counts.sum()
        for i, count in enumerate(target_counts):
            plt.text(i, count + total*0.01, f'{count/total*100:.1f}%', ha='center')
        
        plt.subplot(1, 2, 2)
        target_counts.plot(kind='pie', autopct='%1.1f%%')
        plt.title(f'{self.target_variable} Proportion')
        
        plt.tight_layout()
        plt.savefig('eda_figures/target_distribution.png', dpi=300, bbox_inches='tight')
        plt.show()
        
        # Analyze numeric features vs categorical target
        significant_features = []
        
        for feature in self.numeric_cols:
            if feature != self.target_variable:
                # Group by target and analyze feature
                grouped = self.df.groupby(self.target_variable)[feature].agg(['mean', 'std', 'count'])
                print(f"\n{feature.upper()} by {self.target_variable}:")
                print(grouped)
                
                # Statistical test (ANOVA for multiple groups, t-test for 2 groups)
                groups = [group[feature].dropna() for name, group in self.df.groupby(self.target_variable)]
                
                if len(groups) == 2:
                    stat, p_value = stats.ttest_ind(groups[0], groups[1])
                    test_name = "t-test"
                else:
                    stat, p_value = stats.f_oneway(*groups)
                    test_name = "ANOVA"
                
                print(f"{test_name}: statistic={stat:.3f}, p-value={p_value:.3f}")
                
                if p_value < 0.05:
                    significant_features.append((feature, p_value))
                    print("‚úì Statistically significant difference")
                else:
                    print("‚úó No significant difference")
        
        # Visualize significant features
        if significant_features:
            significant_features.sort(key=lambda x: x[1])  # Sort by p-value
            top_features = [feat for feat, _ in significant_features[:4]]
            
            fig, axes = plt.subplots(2, 2, figsize=(15, 12))
            axes = axes.flatten()
            
            for i, feature in enumerate(top_features):
                if i < 4:
                    sns.boxplot(data=self.df, x=self.target_variable, y=feature, ax=axes[i])
                    axes[i].set_title(f'{feature} by {self.target_variable}')
                    axes[i].tick_params(axis='x', rotation=45)
            
            plt.tight_layout()
            plt.savefig('eda_figures/target_relationships.png', dpi=300, bbox_inches='tight')
            plt.show()

# Execute bivariate analysis
bivariate_analyzer = BivariateAnalyzer(df, '{{target_variable}}' if '{{target_variable}}' else None)
correlation_matrix = bivariate_analyzer.correlation_analysis()
bivariate_analyzer.target_variable_analysis()
```

---

{{#if include_advanced_stats}}
## 4. Advanced Statistical Analysis

```python
class AdvancedStatisticalAnalyzer:
    def __init__(self, df: pd.DataFrame):
        self.df = df
        self.numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()
        self.categorical_cols = df.select_dtypes(include=['object', 'category']).columns.tolist()
        
    def outlier_detection(self):
        """Multi-method outlier detection"""
        print("\nüö® OUTLIER DETECTION")
        print("-" * 40)
        
        if not self.numeric_cols:
            return
            
        outlier_results = {}
        
        for col in self.numeric_cols:
            data = self.df[col].dropna()
            
            # Method 1: IQR
            Q1 = data.quantile(0.25)
            Q3 = data.quantile(0.75)
            IQR = Q3 - Q1
            lower_bound = Q1 - 1.5 * IQR
            upper_bound = Q3 + 1.5 * IQR
            iqr_outliers = ((data < lower_bound) | (data > upper_bound)).sum()
            
            # Method 2: Z-score
            z_scores = np.abs(stats.zscore(data))
            zscore_outliers = (z_scores > 3).sum()
            
            # Method 3: Isolation Forest
            iso_forest = IsolationForest(contamination=0.1, random_state=42)
            outliers_pred = iso_forest.fit_predict(data.values.reshape(-1, 1))
            iso_outliers = (outliers_pred == -1).sum()
            
            outlier_results[col] = {
                'iqr_outliers': iqr_outliers,
                'zscore_outliers': zscore_outliers,
                'isolation_outliers': iso_outliers,
                'total_observations': len(data)
            }
            
            print(f"{col}:")
            print(f"  IQR method: {iqr_outliers} outliers ({iqr_outliers/len(data)*100:.1f}%)")
            print(f"  Z-score method: {zscore_outliers} outliers ({zscore_outliers/len(data)*100:.1f}%)")
            print(f"  Isolation Forest: {iso_outliers} outliers ({iso_outliers/len(data)*100:.1f}%)")
        
        # Visualization
        if len(self.numeric_cols) <= 6:
            fig, axes = plt.subplots(2, 3, figsize=(18, 12))
            axes = axes.flatten() if len(self.numeric_cols) > 3 else axes
            
            for i, col in enumerate(self.numeric_cols[:6]):
                if len(self.numeric_cols) == 1:
                    ax = axes
                else:
                    ax = axes[i]
                    
                data = self.df[col].dropna()
                
                # Box plot with outliers highlighted
                bp = ax.boxplot(data, patch_artist=True)
                bp['boxes'][0].set_facecolor('lightblue')
                
                # Highlight outliers
                Q1 = data.quantile(0.25)
                Q3 = data.quantile(0.75)
                IQR = Q3 - Q1
                lower_bound = Q1 - 1.5 * IQR
                upper_bound = Q3 + 1.5 * IQR
                
                outliers = data[(data < lower_bound) | (data > upper_bound)]
                if len(outliers) > 0:
                    ax.scatter([1] * len(outliers), outliers, color='red', alpha=0.6, s=20)
                
                ax.set_title(f'{col}\n{len(outliers)} outliers')
                ax.set_xlabel('Variable')
                ax.set_ylabel('Value')
            
            plt.tight_layout()
            plt.savefig('eda_figures/outlier_analysis.png', dpi=300, bbox_inches='tight')
            plt.show()
        
        return outlier_results
        
    def dimensionality_analysis(self):
        """PCA and dimensionality reduction analysis"""
        print("\nüìê DIMENSIONALITY ANALYSIS")
        print("-" * 40)
        
        if len(self.numeric_cols) < 3:
            print("Need at least 3 numeric variables for meaningful PCA.")
            return
            
        # Prepare data
        data = self.df[self.numeric_cols].dropna()
        
        if len(data) == 0:
            print("No complete observations available.")
            return
            
        # Standardize data
        scaler = StandardScaler()
        data_scaled = scaler.fit_transform(data)
        
        # PCA
        pca = PCA()
        pca_result = pca.fit_transform(data_scaled)
        
        # Explained variance
        explained_var_ratio = pca.explained_variance_ratio_
        cumulative_var_ratio = np.cumsum(explained_var_ratio)
        
        print(f"Number of original features: {len(self.numeric_cols)}")
        print("Explained variance by component:")
        
        for i, (var_ratio, cum_ratio) in enumerate(zip(explained_var_ratio[:10], cumulative_var_ratio[:10])):
            print(f"  PC{i+1}: {var_ratio:.3f} (cumulative: {cum_ratio:.3f})")
        
        # Find number of components for 95% variance
        n_components_95 = np.argmax(cumulative_var_ratio >= 0.95) + 1
        print(f"\nComponents needed for 95% variance: {n_components_95}")
        
        # Visualization
        fig, axes = plt.subplots(1, 3, figsize=(18, 6))
        
        # Scree plot
        axes[0].plot(range(1, min(21, len(explained_var_ratio) + 1)), 
                    explained_var_ratio[:20], 'bo-')
        axes[0].set_title('Scree Plot')
        axes[0].set_xlabel('Principal Component')
        axes[0].set_ylabel('Explained Variance Ratio')
        axes[0].grid(True)
        
        # Cumulative variance
        axes[1].plot(range(1, min(21, len(cumulative_var_ratio) + 1)), 
                    cumulative_var_ratio[:20], 'ro-')
        axes[1].axhline(y=0.95, color='g', linestyle='--', alpha=0.7, label='95% threshold')
        axes[1].set_title('Cumulative Explained Variance')
        axes[1].set_xlabel('Principal Component')
        axes[1].set_ylabel('Cumulative Explained Variance')
        axes[1].legend()
        axes[1].grid(True)
        
        # Biplot (first 2 components)
        if len(data) > 0:
            axes[2].scatter(pca_result[:, 0], pca_result[:, 1], alpha=0.6)
            axes[2].set_title('PCA Biplot (PC1 vs PC2)')
            axes[2].set_xlabel(f'PC1 ({explained_var_ratio[0]:.1%} variance)')
            axes[2].set_ylabel(f'PC2 ({explained_var_ratio[1]:.1%} variance)')
            
            # Add feature vectors
            feature_vectors = pca.components_[:2].T * np.sqrt(pca.explained_variance_[:2])
            
            for i, (feature, vector) in enumerate(zip(self.numeric_cols, feature_vectors)):
                if i < 10:  # Limit to 10 features for clarity
                    axes[2].arrow(0, 0, vector[0], vector[1], head_width=0.05, 
                                head_length=0.05, fc='red', ec='red', alpha=0.7)
                    axes[2].text(vector[0]*1.1, vector[1]*1.1, feature, 
                               fontsize=8, ha='center', va='center')
        
        plt.tight_layout()
        plt.savefig('eda_figures/pca_analysis.png', dpi=300, bbox_inches='tight')
        plt.show()
        
        return {
            'pca_model': pca,
            'explained_variance_ratio': explained_var_ratio,
            'n_components_95': n_components_95,
            'transformed_data': pca_result
        }
        
    def clustering_analysis(self):
        """Exploratory clustering analysis"""
        print("\nüéØ CLUSTERING ANALYSIS")
        print("-" * 40)
        
        if len(self.numeric_cols) < 2:
            print("Need at least 2 numeric variables for clustering.")
            return
            
        # Prepare data
        data = self.df[self.numeric_cols].dropna()
        
        if len(data) < 10:
            print("Insufficient data for clustering analysis.")
            return
            
        # Standardize data
        scaler = StandardScaler()
        data_scaled = scaler.fit_transform(data)
        
        # Determine optimal number of clusters using elbow method
        max_clusters = min(10, len(data) // 2)
        inertias = []
        silhouette_scores = []
        
        for k in range(2, max_clusters + 1):
            kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)
            kmeans.fit(data_scaled)
            inertias.append(kmeans.inertia_)
            
            # Calculate silhouette score
            from sklearn.metrics import silhouette_score
            sil_score = silhouette_score(data_scaled, kmeans.labels_)
            silhouette_scores.append(sil_score)
        
        # Find optimal k
        optimal_k_silhouette = range(2, max_clusters + 1)[np.argmax(silhouette_scores)]
        
        print(f"Optimal number of clusters (silhouette): {optimal_k_silhouette}")
        
        # Final clustering
        kmeans = KMeans(n_clusters=optimal_k_silhouette, random_state=42, n_init=10)
        cluster_labels = kmeans.fit_predict(data_scaled)
        
        # Visualization
        fig, axes = plt.subplots(1, 3, figsize=(18, 6))
        
        # Elbow curve
        axes[0].plot(range(2, max_clusters + 1), inertias, 'bo-')
        axes[0].set_title('Elbow Method')
        axes[0].set_xlabel('Number of Clusters')
        axes[0].set_ylabel('Inertia')
        axes[0].grid(True)
        
        # Silhouette scores
        axes[1].plot(range(2, max_clusters + 1), silhouette_scores, 'ro-')
        axes[1].set_title('Silhouette Analysis')
        axes[1].set_xlabel('Number of Clusters')
        axes[1].set_ylabel('Silhouette Score')
        axes[1].axvline(x=optimal_k_silhouette, color='g', linestyle='--', alpha=0.7)
        axes[1].grid(True)
        
        # Cluster visualization (first 2 principal components)
        pca_temp = PCA(n_components=2)
        data_pca = pca_temp.fit_transform(data_scaled)
        
        scatter = axes[2].scatter(data_pca[:, 0], data_pca[:, 1], c=cluster_labels, cmap='viridis', alpha=0.6)
        axes[2].scatter(pca_temp.transform(kmeans.cluster_centers_)[:, 0], 
                       pca_temp.transform(kmeans.cluster_centers_)[:, 1], 
                       c='red', marker='x', s=200, linewidths=3)
        axes[2].set_title(f'Clusters (k={optimal_k_silhouette})')
        axes[2].set_xlabel('First Principal Component')
        axes[2].set_ylabel('Second Principal Component')
        plt.colorbar(scatter, ax=axes[2])
        
        plt.tight_layout()
        plt.savefig('eda_figures/clustering_analysis.png', dpi=300, bbox_inches='tight')
        plt.show()
        
        # Cluster characteristics
        cluster_df = data.copy()
        cluster_df['Cluster'] = cluster_labels
        
        print("\nCLUSTER CHARACTERISTICS:")
        cluster_summary = cluster_df.groupby('Cluster').agg(['mean', 'std', 'count']).round(3)
        print(cluster_summary)
        
        return {
            'optimal_k': optimal_k_silhouette,
            'cluster_labels': cluster_labels,
            'silhouette_scores': silhouette_scores,
            'cluster_centers': kmeans.cluster_centers_
        }

# Execute advanced analysis
advanced_analyzer = AdvancedStatisticalAnalyzer(df)
outlier_results = advanced_analyzer.outlier_detection()
pca_results = advanced_analyzer.dimensionality_analysis()
clustering_results = advanced_analyzer.clustering_analysis()
```
{{/if}}

---

{{#if (eq analysis_focus "business-insights")}}
## 5. Business Intelligence Analysis

```python
class BusinessInsightsAnalyzer:
    def __init__(self, df: pd.DataFrame, business_context: str = None):
        self.df = df
        self.business_context = business_context
        
    def generate_business_insights(self):
        """Generate actionable business insights"""
        print("\nüíº BUSINESS INSIGHTS")
        print("-" * 40)
        
        insights = []
        
        # Data volume insights
        total_records = len(self.df)
        insights.append(f"üìä Dataset contains {total_records:,} records")
        
        # Missing data business impact
        missing_data_cols = self.df.columns[self.df.isnull().sum() > 0]
        if len(missing_data_cols) > 0:
            critical_missing = self.df.columns[self.df.isnull().sum() > len(self.df) * 0.3]
            if len(critical_missing) > 0:
                insights.append(f"‚ö†Ô∏è Critical data quality issue: {len(critical_missing)} columns missing >30% data")
            
        # Categorical analysis insights
        for col in self.df.select_dtypes(include=['object', 'category']).columns:
            unique_ratio = self.df[col].nunique() / len(self.df)
            
            if unique_ratio > 0.95:
                insights.append(f"üîç '{col}' appears to be an identifier (95%+ unique values)")
            elif unique_ratio < 0.1:
                insights.append(f"üìà '{col}' has low diversity ({self.df[col].nunique()} unique values)")
                
            # Check for data quality issues
            null_pct = self.df[col].isnull().sum() / len(self.df) * 100
            if null_pct > 20:
                insights.append(f"‚ùó '{col}' has significant missing data ({null_pct:.1f}%)")
        
        # Numeric insights
        for col in self.df.select_dtypes(include=[np.number]).columns:
            # Check for potential issues
            zero_pct = (self.df[col] == 0).sum() / len(self.df) * 100
            negative_pct = (self.df[col] < 0).sum() / len(self.df) * 100
            
            if zero_pct > 50:
                insights.append(f"üìâ '{col}' has high zero percentage ({zero_pct:.1f}%)")
            
            if negative_pct > 0 and col.lower() in ['price', 'cost', 'amount', 'quantity']:
                insights.append(f"‚ö†Ô∏è '{col}' has negative values ({negative_pct:.1f}%) - verify data quality")
        
        # Print insights
        print("KEY BUSINESS INSIGHTS:")
        for i, insight in enumerate(insights, 1):
            print(f"{i}. {insight}")
            
        # Recommendations
        recommendations = self._generate_recommendations()
        
        print("\nüí° RECOMMENDATIONS:")
        for i, rec in enumerate(recommendations, 1):
            print(f"{i}. {rec}")
            
        return insights, recommendations
        
    def _generate_recommendations(self):
        """Generate actionable recommendations"""
        recommendations = []
        
        # Data quality recommendations
        high_missing_cols = self.df.columns[self.df.isnull().sum() > len(self.df) * 0.2]
        if len(high_missing_cols) > 0:
            recommendations.append(f"Address missing data in: {', '.join(high_missing_cols.tolist())}")
            
        # High cardinality recommendations
        high_cardinality_cols = []
        for col in self.df.select_dtypes(include=['object']).columns:
            if self.df[col].nunique() > 100 and self.df[col].nunique() < len(self.df) * 0.95:
                high_cardinality_cols.append(col)
                
        if high_cardinality_cols:
            recommendations.append(f"Consider grouping rare categories in: {', '.join(high_cardinality_cols)}")
            
        # Outlier recommendations
        numeric_cols = self.df.select_dtypes(include=[np.number]).columns
        if len(numeric_cols) > 0:
            recommendations.append("Review outliers identified in numeric variables before modeling")
            
        # Correlation recommendations
        if len(numeric_cols) > 1:
            corr_matrix = self.df[numeric_cols].corr()
            high_corr = np.where((np.abs(corr_matrix) > 0.8) & (corr_matrix != 1))
            
            if len(high_corr[0]) > 0:
                recommendations.append("Consider feature selection due to high multicollinearity")
                
        # Sample size recommendations
        if len(self.df) < 1000:
            recommendations.append("Consider collecting more data - small sample size may limit analysis")
        elif len(self.df) > 100000:
            recommendations.append("Consider sampling strategies for faster analysis and modeling")
            
        return recommendations

# Generate business insights
{{#if business_context}}
insights_analyzer = BusinessInsightsAnalyzer(df, '{{business_context}}')
{{else}}
insights_analyzer = BusinessInsightsAnalyzer(df)
{{/if}}
business_insights, recommendations = insights_analyzer.generate_business_insights()
```
{{/if}}

---

## 6. Summary Report

```python
class EDASummaryReport:
    def __init__(self, df: pd.DataFrame):
        self.df = df
        
    def generate_executive_summary(self):
        """Generate executive summary of EDA findings"""
        print("\nüìã EXECUTIVE SUMMARY")
        print("=" * 50)
        
        # Dataset overview
        print("DATASET OVERVIEW:")
        print(f"‚Ä¢ Shape: {self.df.shape[0]:,} rows √ó {self.df.shape[1]} columns")
        print(f"‚Ä¢ Memory usage: {self.df.memory_usage(deep=True).sum() / 1024**2:.1f} MB")
        
        # Data types breakdown
        dtype_counts = self.df.dtypes.value_counts()
        print(f"‚Ä¢ Data types: {dict(dtype_counts)}")
        
        # Data quality overview
        missing_data_pct = (self.df.isnull().sum().sum() / (len(self.df) * len(self.df.columns))) * 100
        print(f"‚Ä¢ Missing data: {missing_data_pct:.1f}% overall")
        
        complete_rows = len(self.df.dropna())
        print(f"‚Ä¢ Complete records: {complete_rows:,} ({complete_rows/len(self.df)*100:.1f}%)")
        
        # Duplicates
        duplicates = self.df.duplicated().sum()
        print(f"‚Ä¢ Duplicate rows: {duplicates:,} ({duplicates/len(self.df)*100:.1f}%)")
        
        print("\nKEY FINDINGS:")
        
        # Identify potential issues
        issues = []
        
        # High missing data columns
        high_missing = self.df.columns[self.df.isnull().sum() > len(self.df) * 0.3]
        if len(high_missing) > 0:
            issues.append(f"‚Ä¢ {len(high_missing)} columns with >30% missing data")
            
        # High cardinality categorical columns
        high_card_cols = []
        for col in self.df.select_dtypes(include=['object', 'category']).columns:
            if self.df[col].nunique() > 50 and self.df[col].nunique() < len(self.df) * 0.95:
                high_card_cols.append(col)
        
        if high_card_cols:
            issues.append(f"‚Ä¢ {len(high_card_cols)} high-cardinality categorical variables")
            
        # Numeric data issues
        numeric_cols = self.df.select_dtypes(include=[np.number]).columns
        potential_outlier_cols = []
        
        for col in numeric_cols:
            Q1 = self.df[col].quantile(0.25)
            Q3 = self.df[col].quantile(0.75)
            IQR = Q3 - Q1
            lower_bound = Q1 - 1.5 * IQR
            upper_bound = Q3 + 1.5 * IQR
            outliers = ((self.df[col] < lower_bound) | (self.df[col] > upper_bound)).sum()
            
            if outliers > len(self.df) * 0.05:  # >5% outliers
                potential_outlier_cols.append(col)
                
        if potential_outlier_cols:
            issues.append(f"‚Ä¢ {len(potential_outlier_cols)} variables with potential outlier issues")
            
        # High correlation pairs
        if len(numeric_cols) > 1:
            corr_matrix = self.df[numeric_cols].corr()
            high_corr_pairs = 0
            
            for i in range(len(corr_matrix.columns)):
                for j in range(i+1, len(corr_matrix.columns)):
                    if abs(corr_matrix.iloc[i, j]) > 0.8:
                        high_corr_pairs += 1
                        
            if high_corr_pairs > 0:
                issues.append(f"‚Ä¢ {high_corr_pairs} highly correlated variable pairs (|r| > 0.8)")
        
        if issues:
            for issue in issues:
                print(issue)
        else:
            print("‚Ä¢ No major data quality issues identified")
            
        print("\nRECOMMENDATIONS:")
        
        # Generate recommendations based on findings
        recommendations = []
        
        if len(high_missing) > 0:
            recommendations.append("‚Ä¢ Address missing data through imputation or feature engineering")
            
        if high_card_cols:
            recommendations.append("‚Ä¢ Consider feature engineering for high-cardinality categorical variables")
            
        if potential_outlier_cols:
            recommendations.append("‚Ä¢ Investigate and handle outliers in numeric variables")
            
        if len(numeric_cols) > 10:
            recommendations.append("‚Ä¢ Consider dimensionality reduction techniques (PCA, feature selection)")
            
        {{#if (eq analysis_focus "ml-preparation")}}
        recommendations.append("‚Ä¢ Proceed with feature engineering and model selection")
        {{/if}}
        
        {{#if (eq sample_size "small")}}
        recommendations.append("‚Ä¢ Consider data augmentation or collection of additional samples")
        {{else if (eq sample_size "massive")}}
        recommendations.append("‚Ä¢ Implement sampling strategies for efficient model training")
        {{/if}}
        
        if recommendations:
            for rec in recommendations:
                print(rec)
        else:
            print("‚Ä¢ Data appears ready for further analysis")
            
        print("\n" + "=" * 50)

# Generate final summary
summary_report = EDASummaryReport(df)
summary_report.generate_executive_summary()

print(f"""
üéâ EDA COMPLETE for {{dataset_name}}!

üìÅ Generated files:
‚Ä¢ eda_figures/missing_values_heatmap.png
‚Ä¢ eda_figures/numeric_distributions.png  
‚Ä¢ eda_figures/categorical_distributions.png
‚Ä¢ eda_figures/correlation_heatmap.png
{{#if target_variable}}‚Ä¢ eda_figures/target_distribution.png
{{#if (eq problem_type "classification")}}‚Ä¢ eda_figures/target_relationships.png{{else}}‚Ä¢ eda_figures/target_correlations.png{{/if}}{{/if}}
{{#if include_advanced_stats}}‚Ä¢ eda_figures/outlier_analysis.png
‚Ä¢ eda_figures/pca_analysis.png
‚Ä¢ eda_figures/clustering_analysis.png{{/if}}

üìä Next steps:
{{#if (eq analysis_focus "ml-preparation")}}1. Feature engineering based on EDA findings
2. Data preprocessing pipeline
3. Model selection and training{{else if (eq analysis_focus "business-insights")}}1. Present findings to stakeholders
2. Develop action plan based on insights
3. Monitor key metrics identified{{else}}1. Deep dive into specific areas of interest
2. Hypothesis testing for key relationships
3. Advanced modeling or analysis{{/if}}
""")
```

Remember: This EDA provides a comprehensive foundation for {{#if (eq analysis_focus "ml-preparation")}}machine learning projects{{else if (eq analysis_focus "business-insights")}}business decision making{{else}}data-driven insights{{/if}}. Always validate findings with domain expertise and business context.
