---
id: schema-migration-builder
name: Database Schema Migration Builder
version: 1.0.0
description: Generates safe, rollback-ready database migrations with validation and testing
tags: [database, migration, schema, deployment]
parameters:
  - name: migration_name
    type: string
    required: true
    description: Descriptive name for the migration (e.g., "Add user preferences table")
  - name: database_type
    type: string
    enum: [postgresql, mysql, sqlite, mongodb, prisma]
    required: true
    description: Database system being used
  - name: migration_type
    type: string
    enum: [create_table, alter_table, add_column, drop_column, add_index, data_migration, constraint_change]
    required: true
    description: Type of migration being performed
  - name: is_destructive
    type: boolean
    default: false
    description: Migration involves data loss (drops columns, tables, etc.)
  - name: requires_downtime
    type: boolean
    default: false
    description: Migration requires application downtime
  - name: data_volume
    type: string
    enum: [small, medium, large, massive]
    default: small
    description: Amount of data affected by migration
  - name: environment
    type: string
    enum: [development, staging, production]
    required: true
    description: Target environment for migration
---

# Database Migration: {{migration_name}}

## Migration Overview

**Database:** {{database_type}}  
**Migration Type:** {{migration_type}}  
**Environment:** {{environment}}  
**Data Volume:** {{data_volume}}  
**Destructive:** {{#if is_destructive}}⚠️ YES - Data loss possible{{else}}✅ NO{{/if}}  
**Downtime Required:** {{#if requires_downtime}}⚠️ YES{{else}}✅ NO{{/if}}

{{#if is_destructive}}
## ⚠️ DESTRUCTIVE MIGRATION WARNING
This migration may result in permanent data loss. Ensure you have:
- [ ] Complete database backup
- [ ] Tested rollback procedure
- [ ] Business stakeholder approval
- [ ] Maintenance window scheduled
{{/if}}

---

## Pre-Migration Checklist

### Environment Preparation
- [ ] **Database Backup Created**
  - Full database backup completed
  - Backup restoration tested
  - Backup stored in secure location
  - Recovery time objective (RTO) documented

- [ ] **Migration Testing**
  - Migration tested on development environment
  - Migration tested on staging with production data subset
  - Performance impact measured
  - Rollback procedure tested

- [ ] **Monitoring Setup**
  - Database performance monitoring enabled
  - Application error tracking ready
  - Alert thresholds configured
  - Rollback triggers defined

### Risk Assessment
**Risk Level:** {{#if is_destructive}}HIGH{{else if requires_downtime}}MEDIUM{{else if (eq data_volume "massive")}}MEDIUM{{else}}LOW{{/if}}

{{#if (eq environment "production")}}
### Production-Specific Requirements
- [ ] Change management approval obtained
- [ ] Maintenance window scheduled
- [ ] Rollback plan documented and approved
- [ ] Team notification sent
- [ ] On-call engineer identified
{{/if}}

---

## Migration Implementation

{{#if (eq database_type "postgresql")}}
### PostgreSQL Migration

#### Forward Migration
```sql
-- Migration: {{migration_name}}
-- Created: {{now}}
-- Environment: {{environment}}

BEGIN;

{{#if (eq migration_type "create_table")}}
-- Create new table
CREATE TABLE IF NOT EXISTS new_table (
    id BIGSERIAL PRIMARY KEY,
    created_at TIMESTAMPTZ NOT NULL DEFAULT NOW(),
    updated_at TIMESTAMPTZ NOT NULL DEFAULT NOW(),
    
    -- Add your columns here
    name VARCHAR(255) NOT NULL,
    email VARCHAR(255) UNIQUE,
    
    -- Constraints
    CONSTRAINT valid_email CHECK (email ~* '^[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Z|a-z]{2,}$')
);

-- Create indexes
CREATE INDEX CONCURRENTLY IF NOT EXISTS idx_new_table_email ON new_table(email);
CREATE INDEX CONCURRENTLY IF NOT EXISTS idx_new_table_created_at ON new_table(created_at);

-- Add comments
COMMENT ON TABLE new_table IS 'Description of table purpose';
COMMENT ON COLUMN new_table.email IS 'User email address, must be unique';

{{else if (eq migration_type "alter_table")}}
-- Alter existing table
{{#unless requires_downtime}}
-- Non-blocking column addition
ALTER TABLE existing_table ADD COLUMN IF NOT EXISTS new_column TEXT;

-- Add index concurrently (non-blocking)
CREATE INDEX CONCURRENTLY IF NOT EXISTS idx_existing_table_new_column 
ON existing_table(new_column);
{{else}}
-- Blocking alterations (requires downtime)
ALTER TABLE existing_table 
ADD COLUMN new_column TEXT NOT NULL DEFAULT 'default_value',
ADD CONSTRAINT check_new_column CHECK (LENGTH(new_column) > 0);
{{/unless}}

{{else if (eq migration_type "add_column")}}
-- Add new column safely
ALTER TABLE target_table 
ADD COLUMN IF NOT EXISTS new_column_name DATA_TYPE DEFAULT 'safe_default';

-- Update existing records if needed
{{#if (eq data_volume "large")}}
-- For large tables, do this in batches
DO $$
DECLARE
    batch_size INTEGER := 1000;
    processed INTEGER := 0;
BEGIN
    LOOP
        UPDATE target_table 
        SET new_column_name = 'calculated_value'
        WHERE new_column_name IS NULL
        AND id IN (
            SELECT id FROM target_table 
            WHERE new_column_name IS NULL 
            LIMIT batch_size
        );
        
        GET DIAGNOSTICS processed = ROW_COUNT;
        EXIT WHEN processed = 0;
        
        -- Progress logging
        RAISE NOTICE 'Updated % more rows', processed;
        
        -- Pause between batches to avoid blocking
        PERFORM pg_sleep(0.1);
    END LOOP;
END $$;
{{else}}
-- For smaller tables, direct update
UPDATE target_table 
SET new_column_name = 'calculated_value'
WHERE new_column_name IS NULL;
{{/if}}

{{else if (eq migration_type "data_migration")}}
-- Data migration with validation
{{#if (eq data_volume "massive")}}
-- For massive datasets, use batch processing
DO $$
DECLARE
    batch_size INTEGER := 5000;
    total_rows BIGINT;
    processed BIGINT := 0;
    start_time TIMESTAMP := clock_timestamp();
BEGIN
    SELECT COUNT(*) INTO total_rows FROM source_table WHERE migration_needed = true;
    RAISE NOTICE 'Starting migration of % rows', total_rows;
    
    LOOP
        -- Process batch
        WITH batch AS (
            SELECT id, data_field
            FROM source_table 
            WHERE migration_needed = true
            ORDER BY id
            LIMIT batch_size
            FOR UPDATE SKIP LOCKED
        ),
        migrated AS (
            INSERT INTO target_table (source_id, transformed_data)
            SELECT id, transform_function(data_field)
            FROM batch
            RETURNING source_id
        )
        UPDATE source_table 
        SET migration_needed = false
        FROM migrated
        WHERE source_table.id = migrated.source_id;
        
        GET DIAGNOSTICS processed = ROW_COUNT;
        EXIT WHEN processed = 0;
        
        -- Progress reporting
        RAISE NOTICE 'Migrated % of % rows (%.1f%%) - Elapsed: %', 
            processed, total_rows, 
            (processed::FLOAT / total_rows * 100),
            clock_timestamp() - start_time;
            
        -- Breathing room for other operations
        PERFORM pg_sleep(0.5);
    END LOOP;
END $$;
{{else}}
-- Standard data migration
INSERT INTO target_table (source_id, transformed_data)
SELECT id, transform_function(data_field)
FROM source_table 
WHERE needs_migration = true;

-- Mark as migrated
UPDATE source_table 
SET migration_completed = true
WHERE needs_migration = true;
{{/if}}
{{/if}}

-- Verify migration success
DO $$
BEGIN
    -- Add verification queries here
    ASSERT (SELECT COUNT(*) FROM new_table) >= 0, 'Migration verification failed';
    RAISE NOTICE 'Migration verification passed';
END $$;

COMMIT;
```

#### Rollback Migration
```sql
-- Rollback: {{migration_name}}
-- WARNING: This will undo the migration changes

BEGIN;

{{#if (eq migration_type "create_table")}}
-- Drop created table (WARNING: Data loss)
DROP TABLE IF EXISTS new_table CASCADE;

{{else if (eq migration_type "alter_table")}}
-- Reverse table alterations
ALTER TABLE existing_table 
DROP COLUMN IF EXISTS new_column CASCADE;

{{else if (eq migration_type "add_column")}}
-- Remove added column (WARNING: Data loss)
ALTER TABLE target_table 
DROP COLUMN IF EXISTS new_column_name CASCADE;

{{else if (eq migration_type "data_migration")}}
-- Reverse data migration
DELETE FROM target_table 
WHERE source_migration_id = 'current_migration_id';

-- Reset source table migration flags
UPDATE source_table 
SET migration_needed = true, migration_completed = false
WHERE migration_completed = true;
{{/if}}

COMMIT;
```
{{/if}}

{{#if (eq database_type "mongodb")}}
### MongoDB Migration

#### JavaScript Migration Script
```javascript
// Migration: {{migration_name}}
// Database: {{database_type}}
// Created: {{now}}

// Connect to database
use your_database_name;

{{#if (eq migration_type "create_table")}}
// Create collection with validation
db.createCollection("new_collection", {
    validator: {
        $jsonSchema: {
            bsonType: "object",
            required: ["name", "createdAt"],
            properties: {
                name: {
                    bsonType: "string",
                    minLength: 1,
                    maxLength: 255,
                    description: "must be a string between 1-255 characters"
                },
                email: {
                    bsonType: "string",
                    pattern: "^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\.[a-zA-Z]{2,}$",
                    description: "must be a valid email address"
                },
                createdAt: {
                    bsonType: "date",
                    description: "must be a date"
                }
            }
        }
    }
});

// Create indexes
db.new_collection.createIndex({ "email": 1 }, { unique: true, background: true });
db.new_collection.createIndex({ "createdAt": 1 }, { background: true });
db.new_collection.createIndex({ "name": "text" }, { background: true });

{{else if (eq migration_type "alter_table")}}
// Add new field to existing documents
{{#if (eq data_volume "large")}}
// For large collections, use bulk operations
var bulk = db.existing_collection.initializeUnorderedBulkOp();
var count = 0;
var batchSize = 1000;

db.existing_collection.find({ new_field: { $exists: false } }).forEach(function(doc) {
    bulk.find({ _id: doc._id }).updateOne({
        $set: {
            new_field: "default_value",
            updatedAt: new Date()
        }
    });
    
    count++;
    
    if (count % batchSize === 0) {
        bulk.execute();
        bulk = db.existing_collection.initializeUnorderedBulkOp();
        print("Processed " + count + " documents");
    }
});

if (count % batchSize !== 0) {
    bulk.execute();
}
{{else}}
// For smaller collections
db.existing_collection.updateMany(
    { new_field: { $exists: false } },
    {
        $set: {
            new_field: "default_value",
            updatedAt: new Date()
        }
    }
);
{{/if}}

{{else if (eq migration_type "data_migration")}}
// Data transformation migration
{{#if (eq data_volume "massive")}}
// Process in batches for massive datasets
var batchSize = 5000;
var skip = 0;
var total = db.source_collection.count({ needsMigration: true });

print("Starting migration of " + total + " documents");

while (skip < total) {
    var batch = db.source_collection.find({ needsMigration: true })
                                   .skip(skip)
                                   .limit(batchSize)
                                   .toArray();
    
    if (batch.length === 0) break;
    
    var transformedDocs = batch.map(function(doc) {
        return {
            sourceId: doc._id,
            transformedData: transformData(doc.originalData),
            createdAt: new Date(),
            migrationVersion: "{{migration_name}}"
        };
    });
    
    // Insert transformed documents
    db.target_collection.insertMany(transformedDocs);
    
    // Mark as migrated
    var sourceIds = batch.map(function(doc) { return doc._id; });
    db.source_collection.updateMany(
        { _id: { $in: sourceIds } },
        { $set: { needsMigration: false, migratedAt: new Date() } }
    );
    
    skip += batchSize;
    print("Migrated " + skip + " of " + total + " documents");
}

// Custom transformation function
function transformData(originalData) {
    // Implement your transformation logic here
    return {
        newField: originalData.oldField,
        computedValue: originalData.value1 + originalData.value2
    };
}
{{/if}}
{{/if}}

// Verification
var verificationResult = db.runCommand({
    collStats: "new_collection"
});

print("Migration completed successfully");
print("Documents in new collection: " + verificationResult.count);
```

#### Rollback Script
```javascript
// Rollback: {{migration_name}}
use your_database_name;

{{#if (eq migration_type "create_table")}}
// Drop created collection (WARNING: Data loss)
db.new_collection.drop();

{{else if (eq migration_type "alter_table")}}
// Remove added fields
db.existing_collection.updateMany(
    {},
    { $unset: { new_field: "", updatedAt: "" } }
);

{{else if (eq migration_type "data_migration")}}
// Remove migrated data
db.target_collection.deleteMany({
    migrationVersion: "{{migration_name}}"
});

// Reset migration flags
db.source_collection.updateMany(
    { migratedAt: { $exists: true } },
    {
        $set: { needsMigration: true },
        $unset: { migratedAt: "" }
    }
);
{{/if}}

print("Rollback completed");
```
{{/if}}

{{#if (eq database_type "prisma")}}
### Prisma Migration

#### Schema Changes (schema.prisma)
```prisma
{{#if (eq migration_type "create_table")}}
// Add new model
model NewModel {
  id        Int      @id @default(autoincrement())
  createdAt DateTime @default(now())
  updatedAt DateTime @updatedAt
  
  // Add your fields here
  name      String   @db.VarChar(255)
  email     String?  @unique
  
  // Relations
  // posts     Post[]
  
  @@map("new_table")
}
{{else if (eq migration_type "add_column")}}
// Add field to existing model
model ExistingModel {
  id        Int      @id @default(autoincrement())
  // ... existing fields ...
  
  // New field
  newField  String?  @default("default_value")
  
  @@map("existing_table")
}
{{/if}}
```

#### Custom Migration (if needed)
```typescript
// migrations/custom-{{migration_name}}.ts
import { PrismaClient } from '@prisma/client';

const prisma = new PrismaClient();

export async function up(): Promise<void> {
    console.log('Starting migration: {{migration_name}}');
    
    {{#if (eq migration_type "data_migration")}}
    {{#if (eq data_volume "large")}}
    // Process in batches for large datasets
    const batchSize = 1000;
    let skip = 0;
    let hasMore = true;
    
    while (hasMore) {
        const batch = await prisma.sourceTable.findMany({
            where: { needsMigration: true },
            take: batchSize,
            skip: skip,
        });
        
        if (batch.length === 0) {
            hasMore = false;
            break;
        }
        
        // Transform and insert data
        const transformedData = batch.map(item => ({
            sourceId: item.id,
            transformedField: transformData(item.originalField),
        }));
        
        await prisma.targetTable.createMany({
            data: transformedData,
        });
        
        // Mark as migrated
        await prisma.sourceTable.updateMany({
            where: { id: { in: batch.map(item => item.id) } },
            data: { needsMigration: false },
        });
        
        skip += batchSize;
        console.log(`Migrated ${skip} records`);
    }
    {{else}}
    // Direct migration for smaller datasets
    const sourceData = await prisma.sourceTable.findMany({
        where: { needsMigration: true },
    });
    
    const transformedData = sourceData.map(item => ({
        sourceId: item.id,
        transformedField: transformData(item.originalField),
    }));
    
    await prisma.targetTable.createMany({
        data: transformedData,
    });
    
    await prisma.sourceTable.updateMany({
        where: { needsMigration: true },
        data: { needsMigration: false },
    });
    {{/if}}
    {{/if}}
    
    console.log('Migration completed: {{migration_name}}');
}

export async function down(): Promise<void> {
    console.log('Rolling back migration: {{migration_name}}');
    
    // Implement rollback logic
    {{#if (eq migration_type "data_migration")}}
    await prisma.targetTable.deleteMany({
        where: { migrationId: '{{migration_name}}' },
    });
    
    await prisma.sourceTable.updateMany({
        where: { needsMigration: false },
        data: { needsMigration: true },
    });
    {{/if}}
    
    console.log('Rollback completed: {{migration_name}}');
}

function transformData(originalData: any): any {
    // Implement transformation logic
    return originalData;
}
```
{{/if}}

---

## Migration Execution Plan

### Pre-Execution Steps
1. **Backup Database**
   ```bash
   {{#if (eq database_type "postgresql")}}
   pg_dump -h localhost -U username -d database_name > backup_$(date +%Y%m%d_%H%M%S).sql
   {{else if (eq database_type "mysql")}}
   mysqldump -u username -p database_name > backup_$(date +%Y%m%d_%H%M%S).sql
   {{else if (eq database_type "mongodb")}}
   mongodump --db database_name --out backup_$(date +%Y%m%d_%H%M%S)
   {{/if}}
   ```

2. **Test Migration on Copy**
   - Restore backup to test environment
   - Run migration script
   - Validate data integrity
   - Test application functionality

3. **Performance Testing**
   - Measure migration execution time
   - Monitor resource usage
   - Verify indexes are used effectively

### Execution Steps
{{#if (eq environment "production")}}
{{#if requires_downtime}}
1. **Maintenance Mode**
   - Enable maintenance page
   - Stop application servers
   - Wait for active connections to close
{{/if}}
{{/if}}

2. **Run Migration**
   ```bash
   {{#if (eq database_type "prisma")}}
   npx prisma migrate deploy
   {{else if (eq database_type "postgresql")}}
   psql -h localhost -U username -d database_name -f migration.sql
   {{else if (eq database_type "mysql")}}
   mysql -u username -p database_name < migration.sql
   {{else if (eq database_type "mongodb")}}
   mongo database_name migration.js
   {{/if}}
   ```

3. **Verify Migration**
   - Check migration completed successfully
   - Verify data integrity
   - Run application smoke tests

4. **Monitor Application**
   - Watch error rates
   - Monitor performance metrics
   - Check database query performance

{{#if (eq environment "production")}}
{{#if requires_downtime}}
5. **Resume Operations**
   - Start application servers
   - Disable maintenance mode
   - Monitor for errors
{{/if}}
{{/if}}

### Rollback Procedure
If issues are detected:

1. **Immediate Actions**
   - Stop additional traffic if possible
   - Assess the scope of the problem
   - Make rollback decision quickly

2. **Execute Rollback**
   ```bash
   {{#if (eq database_type "postgresql")}}
   psql -h localhost -U username -d database_name -f rollback.sql
   {{else if (eq database_type "mongodb")}}
   mongo database_name rollback.js
   {{/if}}
   ```

3. **Verify Rollback**
   - Confirm data is in previous state
   - Test critical application functionality
   - Monitor for stability

---

## Post-Migration Checklist

### Immediate Verification (0-1 hour)
- [ ] Migration completed without errors
- [ ] Application starts successfully
- [ ] Critical user flows tested
- [ ] Database performance within normal range
- [ ] No error spikes in monitoring

### Extended Verification (24 hours)
- [ ] All application features working
- [ ] Performance metrics stable
- [ ] No data corruption detected
- [ ] User-reported issues addressed
- [ ] Backup integrity verified

### Documentation Updates
- [ ] Migration logged in change management system
- [ ] Database schema documentation updated
- [ ] Application documentation updated if needed
- [ ] Runbook updated with lessons learned

{{#if (eq data_volume "massive")}}
### Performance Optimization (Post-Migration)
- [ ] Analyze query performance impact
- [ ] Update/rebuild indexes if needed
- [ ] Update table statistics
- [ ] Consider additional optimizations
{{/if}}

---

## Monitoring and Alerting

### Key Metrics to Watch
- Database connection count
- Query execution time
- Lock wait time
- Disk I/O utilization
- Application error rate
- Response time

### Alert Conditions
```yaml
# Example monitoring alerts
- alert: MigrationPerformanceImpact
  expr: database_query_duration_seconds > baseline * 1.5
  for: 5m
  
- alert: ApplicationErrorSpike  
  expr: application_error_rate > baseline * 2
  for: 2m

- alert: DatabaseConnectionExhaustion
  expr: database_connections_active > database_connections_max * 0.8
  for: 1m
```

Remember: Migrations are irreversible in production. Always test thoroughly and have a rollback plan ready. When in doubt, choose the safer, more conservative approach.